{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b01fc1-04bf-4a90-9d40-a4ea8f5d3b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from clinical_codes import map, load_map\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "import random\n",
    "from logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc08734-4046-4533-9975-0c6f245bd73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9760c0d2-cf1e-40c9-90c0-f018560864f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClaimLine:\n",
    " \"\"\"Represents a single claim line in Medicare claims data.\"\"\"\n",
    " claim_id: str\n",
    " service_line_num: int\n",
    " procedure_code: str\n",
    " procedure_description: str\n",
    " modifiers: List[str]\n",
    " modifier_descriptions: List[str]\n",
    " diagnosis_codes: List[str]\n",
    " diagnosis_codes_descriptions: List[str]\n",
    " amount: float\n",
    " provider_specialty: str\n",
    " provider_specialty_description: str\n",
    "\n",
    " date_of_service: str\n",
    " place_of_service: str\n",
    " approval_status: str = \"UNKNOWN\"  # New field to track if claim was approved/denied\n",
    " revenue_code: str = None\n",
    "\n",
    " @property\n",
    " def is_ambulance_claim(self) -> bool:\n",
    "     \"\"\"Check if claim is ambulance claim.\"\"\"\n",
    "     ambulance_hcpcs = [\"A0426\", \"A0427\", \"A0428\", \"A0429\", \"A0430\", \"A0431\", \n",
    "                          \"A0432\", \"A0433\", \"A0434\", \"A0380\", \"A0390\", \"A0435\", \"A0436\",\n",
    "                          \"A0030\", \"A0040\", \"A0050\", \"A0320\", \"A0322\", \"A0324\", \n",
    "                          \"A0326\", \"A0328\", \"A0330\"]\n",
    "     return self.procedure_code in ambulance_hcpcs\n",
    "\n",
    " @property\n",
    " def has_valid_ambulance_revenue_code(self) -> bool:\n",
    "     \"\"\"Check if revenue code is valid for ambulance services.\"\"\"\n",
    "     valid_codes = [\"0540\", \"0542\", \"0543\", \"0545\", \"0546\", \"0548\"]\n",
    "     invalid_codes = [\"0541\", \"0544\", \"0547\"]\n",
    "        \n",
    "     if self.revenue_code in invalid_codes:\n",
    "         return False\n",
    "        \n",
    "     # For claims after Jan 1, 2001, only 0540 is valid\n",
    "     if self.date_of_service > \"2001-01-01\" and self.revenue_code != \"0540\":\n",
    "         return False\n",
    "            \n",
    "     return self.revenue_code in valid_codes\n",
    "    \n",
    " @property\n",
    " def has_death_pronouncement_modifier(self) -> bool:\n",
    "     \"\"\"Check if this claim has the QL modifier for death pronouncement.\"\"\"\n",
    "     return \"QL\" in self.modifiers\n",
    " \n",
    " @property\n",
    " def has_anatomic_modifiers(self) -> bool:\n",
    "     \"\"\"Check if claim has anatomic modifiers (RT, LT).\"\"\"\n",
    "     return any(mod in [\"RT\", \"LT\"] for mod in self.modifiers)\n",
    " \n",
    " @property\n",
    " def has_bypass_modifiers(self) -> bool:\n",
    "     \"\"\"Check if claim has bypass modifiers (GX, GY, GZ, RB).\"\"\"\n",
    "     return any(mod in [\"GX\", \"GY\", \"GZ\", \"RB\"] for mod in self.modifiers)\n",
    " \n",
    " @property\n",
    " def has_kx_modifier(self) -> bool:\n",
    "     \"\"\"Check if claim has KX modifier which can override certain edits.\"\"\"\n",
    "     return \"KX\" in self.modifiers\n",
    " \n",
    " @property\n",
    " def has_59_modifier(self) -> bool:\n",
    "     \"\"\"Check if claim has modifier 59 which indicates distinct procedural service.\"\"\"\n",
    "     return \"59\" in self.modifiers\n",
    " \n",
    " def has_bypass_modifier_for_ncci(self) -> bool:\n",
    "     \"\"\"Check if claim has any modifier that can bypass NCCI edits.\"\"\"\n",
    "     # Common modifiers that can bypass NCCI edits\n",
    "     # bypass_modifiers = [\"59\", \"XE\", \"XP\", \"XS\", \"XU\", \"25\", \"91\"]\n",
    "     bypass_modifiers = []\n",
    "     return any(mod in bypass_modifiers for mod in self.modifiers)\n",
    " \n",
    " def to_prompt_format(self) -> str:\n",
    "     \"\"\"Format claim data for inclusion in a prompt.\"\"\"\n",
    "     return f\"\"\"Claim ID: {self.claim_id}\n",
    "Date of Service: {self.date_of_service}\n",
    "Provider Type: {self.provider_specialty}\n",
    "Procedure Codes: {self.procedure_code}\n",
    "Modifiers: {', '.join(self.modifiers) if self.modifiers else 'None'}\n",
    "Diagnosis Codes: {', '.join(self.diagnosis_codes)}\n",
    "Place of Service: {self.place_of_service}\n",
    "Amount: ${self.amount:.2f}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b386b8d-9631-4caa-9c14-e3969b24b7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MedicareClaimsDataset:\n",
    " \"\"\"Handles loading and preprocessing of Medicare claims data.\"\"\"\n",
    " \n",
    " def __init__(self, data_path: str):\n",
    "     \"\"\"Initialize the dataset with a path to the data file.\"\"\"\n",
    "     self.data_path = data_path\n",
    "     self.claims = []\n",
    "     \n",
    "     self.diagnosis_mappings = load_map(\"DGNS.csv\")\n",
    "     self.provider_mappings = load_map(\"PRVDR.csv\")\n",
    "     self.hcpcs_mappings = load_map(\"HCPCS.csv\")\n",
    "     self.modifier_mappings = load_map(\"MODIFIERS.csv\")\n",
    "     self.load_data()\n",
    "     \n",
    " \n",
    " def load_data(self):\n",
    "    \"\"\"Load claims data from various possible formats.\"\"\"\n",
    "    logger.info(f\"Loading claims data from {self.data_path}\")\n",
    "    \n",
    "    file_extension = Path(self.data_path).suffix.lower()\n",
    "    \n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            self._load_from_csv()\n",
    "        elif file_extension == '.json':\n",
    "            self._load_from_json()\n",
    "        elif file_extension == '.parquet':\n",
    "            self._load_from_parquet()\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            self._load_from_excel()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "        \n",
    "        # Group claim lines by claim ID after loading\n",
    "        self._associate_claim_lines()\n",
    "            \n",
    "        logger.info(f\"Successfully loaded {len(self.claims)} claim lines\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading claims data: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    " def _associate_claim_lines(self):\n",
    "     \"\"\"Associate claim lines with the same claim ID with each other.\"\"\"\n",
    "     claims_by_id = {}\n",
    "     for claim in self.claims:\n",
    "         if claim.claim_id not in claims_by_id:\n",
    "             claims_by_id[claim.claim_id] = []\n",
    "         claims_by_id[claim.claim_id].append(claim)\n",
    "        \n",
    "     # Update each claim with its associated claim lines\n",
    "     for claim in self.claims:\n",
    "         claim.associated_claim_lines = [\n",
    "             c for c in claims_by_id[claim.claim_id] if c != claim\n",
    "         ]\n",
    "    \n",
    "     # Print statistics about associated claim lines\n",
    "     claims_with_associates = sum(1 for c in self.claims if c.associated_claim_lines)\n",
    "     avg_associates = sum(len(c.associated_claim_lines) for c in self.claims) / len(self.claims) if self.claims else 0\n",
    "     print(f\"Claims with associated lines: {claims_with_associates}\")\n",
    "     print(f\"Average associated lines per claim: {avg_associates:.2f}\")\n",
    "\n",
    "\n",
    " def load_ambulance_csv(self, file_path: str):\n",
    "     \"\"\"Load ambulance claims data with specific structure.\"\"\"\n",
    "     print(f\"Loading ambulance data from {file_path}\")\n",
    "     df = pd.read_csv(file_path)\n",
    "    \n",
    "     # Group by claim ID\n",
    "     claims_grouped = df.groupby('claim_id')\n",
    "     print(f\"Found {len(claims_grouped)} unique claims\")\n",
    "    \n",
    "     for claim_id, group in claims_grouped:\n",
    "         claim_lines = []\n",
    "        \n",
    "         for _, row in group.iterrows():\n",
    "             # Extract procedure code (HCPCS)\n",
    "             procedure_code = str(row['hcpcs']) if 'hcpcs' in row.index else \"\"\n",
    "            \n",
    "             # Skip if no procedure code\n",
    "             if not procedure_code or pd.isna(procedure_code):\n",
    "                 continue\n",
    "                \n",
    "             # Extract modifiers\n",
    "             modifiers = []\n",
    "             for mod_col in ['mod1', 'mod2', 'mod3', 'mod4']:\n",
    "                 if mod_col in row.index and pd.notna(row[mod_col]) and row[mod_col]:\n",
    "                     modifiers.append(str(row[mod_col]))\n",
    "            \n",
    "             # Create claim line\n",
    "             claim_line = ClaimLine(\n",
    "                 claim_id=str(claim_id),\n",
    "                 service_line_num=int(row['line_num']) if 'line_num' in row.index else 0,\n",
    "                 procedure_code=procedure_code,\n",
    "                 procedure_description=map(procedure_code, self.hcpcs_mappings),\n",
    "                 modifiers=modifiers,\n",
    "                 modifiers_descriptions=map(modifiers, self.modifier_mappings),\n",
    "                 diagnosis_codes=[],\n",
    "                 diagnosis_codes_descriptions=map(diagnosis_codes, self.diagnosis_mappings),\n",
    "                 amount=float(row['charge']) if 'charge' in row.index and pd.notna(row['charge']) else 0.0,\n",
    "                 provider_specialty=\"Ambulance\",\n",
    "                 provider_specialty_description=map(provider_specialty, self.provider_mappings),\n",
    "                 date_of_service=str(row['service_date']) if 'service_date' in row.index and pd.notna(row['service_date']) else \"2025-01-01\",\n",
    "                 place_of_service=\"\",\n",
    "                 approval_status=\"APPROVED\",  # Default for testing\n",
    "                 revenue_code=str(row['rev_code']) if 'rev_code' in row.index and pd.notna(row['rev_code']) else None\n",
    "             )\n",
    "             claim_lines.append(claim_line)\n",
    "            \n",
    "         # Associate claim lines with each other\n",
    "         for claim_line in claim_lines:\n",
    "             claim_line.associated_claim_lines = [cl for cl in claim_lines if cl != claim_line]\n",
    "             self.claims.append(claim_line)\n",
    "        \n",
    "     # Print statistics\n",
    "     ambulance_codes = [\"A0426\", \"A0427\", \"A0428\", \"A0429\", \"A0430\", \"A0431\", \"A0432\", \"A0433\", \"A0434\"]\n",
    "     mileage_codes = [\"A0380\", \"A0390\", \"A0435\", \"A0436\", \"A0425\"]\n",
    "    \n",
    "     transport_claims = sum(1 for c in self.claims if c.procedure_code in ambulance_codes)\n",
    "     mileage_claims = sum(1 for c in self.claims if c.procedure_code in mileage_codes)\n",
    "    \n",
    "     print(f\"Loaded {len(self.claims)} total claim lines\")\n",
    "     print(f\"  - Transport code lines: {transport_claims}\")\n",
    "     print(f\"  - Mileage code lines: {mileage_claims}\")\n",
    "     print(f\"  - Claims with revenue codes: {sum(1 for c in self.claims if c.revenue_code is not None)}\")\n",
    "\n",
    " def _load_from_csv(self):\n",
    "    \"\"\"Load claims from a CSV file with special handling for the FPS88 claims format.\"\"\"\n",
    "    print(f\"Loading data from {self.data_path}\")\n",
    "    df = pd.read_csv(self.data_path)\n",
    "    \n",
    "    # Debug data structure\n",
    "    print(f\"CSV columns count: {len(df.columns)}\")\n",
    "    columns_sample = df.columns.tolist()[:10]  # First 10 column names\n",
    "    print(f\"First 10 columns: {columns_sample}\")\n",
    "    clmline_status = 'clmline_status'\n",
    "    revenue_code_col = None\n",
    "    for col in df.columns:\n",
    "        if  'REV' in col.upper() or 'REVENUE' in col.upper() or 'REV_CD' in col.upper() or 'REVENUECD' in col.upper():\n",
    "            revenue_code_col = col\n",
    "            break\n",
    "    \n",
    "    # Find procedure code column\n",
    "    proc_code_col = None\n",
    "    for column_name in ['procCd', 'procedure_code', 'PRCDR_CD', 'proc_cd', 'hcpcs_cd']:\n",
    "        if column_name in df.columns:\n",
    "            proc_code_col = column_name\n",
    "            break\n",
    "    \n",
    "    if not proc_code_col:\n",
    "        # Look for L-codes in the dataset\n",
    "        for col in df.columns:\n",
    "            sample_values = df[col].astype(str).head(50).tolist()\n",
    "            if any(str(val).startswith('L') for val in sample_values if pd.notna(val) and str(val) != 'nan'):\n",
    "                proc_code_col = col\n",
    "                print(f\"Found procedure code column by L-code detection: {proc_code_col}\")\n",
    "                break\n",
    "    \n",
    "    if not proc_code_col and len(df.columns) > 51:\n",
    "        proc_code_col = df.columns[51]  # Try position 51 as fallback\n",
    "        print(f\"Using fallback procedure code column: {proc_code_col}\")\n",
    "    \n",
    "    if not proc_code_col:\n",
    "        print(\"ERROR: Could not identify procedure code column!\")\n",
    "        return\n",
    "    \n",
    "    # Find modifier columns\n",
    "    modifier_cols = []\n",
    "    for name in ['procModifier1', 'procModifier2', 'procModifier3', 'procModifier4', 'procModifier5',\n",
    "                'PRCDR_1_MDFR_TXT', 'PRCDR_2_MDFR_TXT', 'PRCDR_3_MDFR_TXT', 'PRCDR_4_MDFR_TXT', 'PRCDR_5_MDFR_TXT']:\n",
    "        if name in df.columns:\n",
    "            modifier_cols.append(name)\n",
    "    \n",
    "    if not modifier_cols:\n",
    "        # Try to find modifier columns by pattern\n",
    "        for col in df.columns:\n",
    "            if 'MDFR' in col or 'modifier' in col.lower() or 'MOD' in col:\n",
    "                modifier_cols.append(col)\n",
    "    \n",
    "    print(f\"Using procedure code column: {proc_code_col}\")\n",
    "    print(f\"Using modifier columns: {modifier_cols}\")\n",
    "    \n",
    "    # Find diagnosis code columns\n",
    "    diag_cols = []\n",
    "    for name in ['diagCd1', 'diagCd2', 'diagCd3', 'DGNS_1_CD', 'DGNS_2_CD', 'DGNS_3_CD']:\n",
    "        if name in df.columns:\n",
    "            diag_cols.append(name)\n",
    "    \n",
    "    if not diag_cols:\n",
    "        # Try to find diagnosis columns by pattern\n",
    "        for col in df.columns:\n",
    "            if 'DGNS' in col or 'diag' in col.lower():\n",
    "                diag_cols.append(col)\n",
    "    \n",
    "    # Find status column (usually the last column)\n",
    "    status_col = None\n",
    "    last_col = df.columns[-1]\n",
    "    sample_values = df[last_col].dropna().astype(str).head(10).tolist()\n",
    "    if any(val in ['APPROVED', 'DENIED'] for val in sample_values):\n",
    "        status_col = last_col\n",
    "        print(f\"Found approval status column: {status_col}\")\n",
    "    \n",
    "    # Process each row into a ClaimLine object\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Get claim ID\n",
    "            claim_id = None\n",
    "            for id_col in ['clm_id', 'claimId', 'ENHCNCMT_ID', 'claim_id']:\n",
    "                if id_col in row.index and pd.notna(row[id_col]):\n",
    "                    claim_id = str(row[id_col])\n",
    "                    break\n",
    "            if not claim_id:\n",
    "                claim_id = f\"CLAIM_{idx}\"\n",
    "            \n",
    "            # Get service line number\n",
    "            service_line_num = idx + 1  # Default\n",
    "            for line_col in ['serviceLineNum', 'SRVC_LINE_NUM', 'service_line_num']:\n",
    "                if line_col in row.index and pd.notna(row[line_col]):\n",
    "                    service_line_num = int(row[line_col])\n",
    "                    break\n",
    "            \n",
    "            # Get procedure code\n",
    "            procedure_code = str(row[proc_code_col]) if pd.notna(row[proc_code_col]) else \"\"\n",
    "            \n",
    "            # Get modifiers from all modifier columns\n",
    "            modifiers = []\n",
    "            for col in modifier_cols:\n",
    "                if col in row.index and pd.notna(row[col]) and row[col]:\n",
    "                    modifiers.append(str(row[col]))\n",
    "            \n",
    "            # Get diagnosis codes\n",
    "            diagnosis_codes = []\n",
    "            for col in diag_cols:\n",
    "                if col in row.index and pd.notna(row[col]) and row[col]:\n",
    "                    diagnosis_codes.append(str(row[col]))\n",
    "            \n",
    "            # Get amount\n",
    "            amount = 0.0\n",
    "            for amt_col in ['ALOWD_CHRG_AMT', 'amount', 'allowChgAmt']:\n",
    "                if amt_col in row.index and pd.notna(row[amt_col]):\n",
    "                    amount = float(row[amt_col])\n",
    "                    break\n",
    "            \n",
    "            # Get revenue code\n",
    "            revenue_code_value = None\n",
    "            if revenue_code_col and revenue_code_col in row.index:\n",
    "                revenue_code_value = str(row[revenue_code_col]) if pd.notna(row[revenue_code_col]) else None\n",
    "            \n",
    "            # Get provider specialty\n",
    "            provider_specialty = \"Unknown\"\n",
    "            for spec_col in ['provSpecCd', 'provider_specialty', 'PRVDR_SPCLTY_CD']:\n",
    "                if spec_col in row.index and pd.notna(row[spec_col]):\n",
    "                    provider_specialty = str(row[spec_col])\n",
    "                    break\n",
    "            \n",
    "            # Get date of service\n",
    "            date_of_service = None\n",
    "            for date_col in ['enhancementTime', 'date_of_service', 'serviceFromDt', 'ENHNCMT_TIME']:\n",
    "                if date_col in row.index and pd.notna(row[date_col]):\n",
    "                    date_str = str(row[date_col])\n",
    "                    # Handle timestamp format\n",
    "                    if 'T' in date_str:\n",
    "                        date_of_service = date_str.split('T')[0]\n",
    "                    else:\n",
    "                        date_of_service = date_str\n",
    "                    break\n",
    "            \n",
    "            if date_of_service is None:\n",
    "                date_of_service = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # Get place of service\n",
    "            place_of_service = \"11\"  # Default\n",
    "            for pos_col in ['placeServCd', 'place_of_service', 'PLC_OD_SRVC_CD']:\n",
    "                if pos_col in row.index and pd.notna(row[pos_col]):\n",
    "                    place_of_service = str(row[pos_col])\n",
    "                    break\n",
    "            \n",
    "            # Get approval status\n",
    "            approval_status = \"UNKNOWN\"\n",
    "            if status_col and pd.notna(row[status_col]):\n",
    "                status_value = str(row[status_col]).strip().upper()\n",
    "                if status_value in [\"APPROVED\", \"DENIED\"]:\n",
    "                    approval_status = status_value\n",
    "            \n",
    "            # Create ClaimLine object\n",
    "            claim_line = ClaimLine(\n",
    "                claim_id=claim_id,\n",
    "                service_line_num=service_line_num,\n",
    "                procedure_code=procedure_code,\n",
    "                procedure_description=map(procedure_code, self.hcpcs_mappings),\n",
    "                modifiers=modifiers,\n",
    "                modifier_descriptions=map(modifiers, self.modifier_mappings),\n",
    "                diagnosis_codes=diagnosis_codes,\n",
    "                diagnosis_codes_descriptions=map(diagnosis_codes, self.diagnosis_mappings),\n",
    "                amount=amount,\n",
    "                provider_specialty=provider_specialty,\n",
    "                provider_specialty_description=map(provider_specialty, self.provider_mappings),\n",
    "                date_of_service=date_of_service,\n",
    "                place_of_service=place_of_service,\n",
    "                approval_status=approval_status,\n",
    "                revenue_code=revenue_code_value\n",
    "            )\n",
    "            self.claims.append(claim_line)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(self.claims)} claim lines\")\n",
    "    \n",
    "    # Analyze the loaded data\n",
    "    procedures = set(c.procedure_code for c in self.claims)\n",
    "    l_codes = [c.procedure_code for c in self.claims if c.procedure_code.startswith('L')]\n",
    "    approved_claims = sum(1 for c in self.claims if c.approval_status == \"APPROVED\")\n",
    "    denied_claims = sum(1 for c in self.claims if c.approval_status == \"DENIED\")\n",
    "    rt_lt_claims = sum(1 for c in self.claims if c.has_anatomic_modifiers)\n",
    "    bypass_claims = sum(1 for c in self.claims if c.has_bypass_modifiers)\n",
    "    ncci_bypass_claims = sum(1 for c in self.claims if hasattr(c, 'has_bypass_modifier_for_ncci') and callable(getattr(c, 'has_bypass_modifier_for_ncci')) and c.has_bypass_modifier_for_ncci())\n",
    "    \n",
    "    print(f\"Unique procedure codes: {len(procedures)}\")\n",
    "    print(f\"L-codes found: {len(set(l_codes))} unique codes\")\n",
    "    if l_codes:\n",
    "        print(f\"Sample L-codes: {sorted(list(set(l_codes)))[:5]}\")\n",
    "    print(f\"Approved claims: {approved_claims}, Denied claims: {denied_claims}\")\n",
    "    print(f\"Claims with RT/LT modifiers: {rt_lt_claims}\")\n",
    "    print(f\"Claims with standard bypass modifiers (GX, GY, GZ, RB): {bypass_claims}\")\n",
    "    print(f\"Claims with NCCI bypass modifiers (59, XE, etc.): {ncci_bypass_claims}\")\n",
    "\n",
    " def _load_from_json(self):\n",
    "     \"\"\"Load claims from a JSON file.\"\"\"\n",
    "     df = pd.read_json(self.data_path)\n",
    "     self._process_dataframe(df)\n",
    " \n",
    " def _load_from_parquet(self):\n",
    "     \"\"\"Load claims from a Parquet file.\"\"\"\n",
    "     df = pd.read_parquet(self.data_path)\n",
    "     self._process_dataframe(df)\n",
    " \n",
    " def _load_from_excel(self):\n",
    "     \"\"\"Load claims from an Excel file.\"\"\"\n",
    "     df = pd.read_excel(self.data_path)\n",
    "     self._process_dataframe(df)\n",
    " \n",
    " def _process_dataframe(self, df: pd.DataFrame):\n",
    "     \"\"\"Process a DataFrame into ClaimLine objects.\"\"\"\n",
    "     # Map DataFrame columns to ClaimLine attributes\n",
    "     # Handle variations in column naming\n",
    "     column_mappings = {\n",
    "         'claim_id': ['claim_id', 'clm_id', 'clm_ID', 'ENHCNCMT_ID'],\n",
    "         'service_line_num': ['service_line_num', 'line_num', 'SRVC_LINE_NUM'],\n",
    "         'procedure_code': ['procedure_code', 'hcpcs_cd', 'hipps_cd', 'cpt_code', 'PRCDR_CD'],\n",
    "         'modifiers': ['modifiers', 'mod', 'modifier', 'PRCDR_1_MDFR_TXT', 'PRCDR_2_MDFR_TXT', 'PRCDR_3_MDFR_TXT'],\n",
    "         'diagnosis_codes': ['diagnosis_codes', 'diag_cd', 'icd_code'],\n",
    "         'amount': ['amount', 'charge_amt', 'ALOWD_CHRG_AMT'],\n",
    "         'provider_specialty': ['provider_specialty', 'prvdr_spclty'],\n",
    "         'date_of_service': ['date_of_service', 'srvc_dt', 'ENHNCMT_TIME'],\n",
    "         'place_of_service': ['place_of_service', 'pos_cd'],\n",
    "         'approval_status': ['approval_status', 'status', 'claim_status']\n",
    "     }\n",
    "     # Standardize column names\n",
    "     standardized_columns = {}\n",
    "     for standard_name, variations in column_mappings.items():\n",
    "         for variation in variations:\n",
    "             if variation in df.columns:\n",
    "                 standardized_columns[variation] = standard_name\n",
    "                 break\n",
    "     \n",
    "     df = df.rename(columns=standardized_columns)\n",
    "     \n",
    "     # Fill missing columns with default values\n",
    "     required_columns = ['claim_id', 'service_line_num', 'procedure_code']\n",
    "     for col in required_columns:\n",
    "         if col not in df.columns:\n",
    "             raise ValueError(f\"Required column '{col}' not found in data\")\n",
    "     \n",
    "     # Handle optional columns\n",
    "     if 'modifiers' not in df.columns:\n",
    "         df['modifiers'] = None\n",
    "     if 'diagnosis_codes' not in df.columns:\n",
    "         df['diagnosis_codes'] = None\n",
    "     if 'amount' not in df.columns:\n",
    "         df['amount'] = 0.0\n",
    "     if 'provider_specialty' not in df.columns:\n",
    "         df['provider_specialty'] = \"Unknown\"\n",
    "     if 'date_of_service' not in df.columns:\n",
    "         df['date_of_service'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "     if 'place_of_service' not in df.columns:\n",
    "         df['place_of_service'] = \"11\"  # Office\n",
    "     if 'approval_status' not in df.columns:\n",
    "         df['approval_status'] = \"UNKNOWN\"\n",
    "     \n",
    "     # Process each row into a ClaimLine object\n",
    "     for _, row in df.iterrows():\n",
    "         # Handle different formats of modifiers\n",
    "         if pd.isna(row['modifiers']) or row['modifiers'] is None:\n",
    "             modifiers = []\n",
    "         elif isinstance(row['modifiers'], str):\n",
    "             if ',' in row['modifiers']:\n",
    "                 modifiers = [m.strip() for m in row['modifiers'].split(',')]\n",
    "             else:\n",
    "                 modifiers = [row['modifiers'].strip()]\n",
    "         else:\n",
    "             modifiers = [str(row['modifiers'])]\n",
    "         \n",
    "         # Handle different formats of diagnosis codes\n",
    "         if pd.isna(row['diagnosis_codes']) or row['diagnosis_codes'] is None:\n",
    "             diagnosis_codes = []\n",
    "         elif isinstance(row['diagnosis_codes'], str):\n",
    "             if ',' in row['diagnosis_codes']:\n",
    "                 diagnosis_codes = [d.strip() for d in row['diagnosis_codes'].split(',')]\n",
    "             else:\n",
    "                 diagnosis_codes = [row['diagnosis_codes'].strip()]\n",
    "         else:\n",
    "             diagnosis_codes = [str(row['diagnosis_codes'])]\n",
    "         \n",
    "         # Get approval status\n",
    "         if pd.isna(row['approval_status']) or row['approval_status'] is None:\n",
    "             approval_status = \"UNKNOWN\"\n",
    "         else:\n",
    "             status = str(row['approval_status']).upper().strip()\n",
    "             if status in [\"APPROVED\", \"DENIED\"]:\n",
    "                 approval_status = status\n",
    "             else:\n",
    "                 approval_status = \"UNKNOWN\"\n",
    "         \n",
    "         claim_line = ClaimLine(\n",
    "             claim_id=str(row['claim_id']),\n",
    "             service_line_num=int(row['service_line_num']),\n",
    "             procedure_code=str(row['procedure_code']),\n",
    "             procedure_description=List(str),\n",
    "             modifiers=modifiers,\n",
    "             modifier_descriptions=List(str),\n",
    "             diagnosis_codes=diagnosis_codes,\n",
    "             diagnosis_mappings=List(str),\n",
    "             amount=float(row['amount']),\n",
    "             provider_specialty=str(row['provider_specialty']),\n",
    "             provider_specialty_description=List(str),\n",
    "             date_of_service=str(row['date_of_service']),\n",
    "             place_of_service=str(row['place_of_service']),\n",
    "             approval_status=approval_status\n",
    "         )\n",
    "         self.claims.append(claim_line)\n",
    " \n",
    " def filter_claims(self, **kwargs) -> List[ClaimLine]:\n",
    "     \"\"\"Filter claims based on criteria.\"\"\"\n",
    "     filtered_claims = self.claims\n",
    "     \n",
    "     for key, value in kwargs.items():\n",
    "         if hasattr(ClaimLine, key):\n",
    "             filtered_claims = [claim for claim in filtered_claims \n",
    "                              if getattr(claim, key) == value]\n",
    "     \n",
    "     return filtered_claims\n",
    " \n",
    " def get_orthotic_claims(self) -> List[ClaimLine]:\n",
    "     \"\"\"Get claims related to orthotics (L-codes).\"\"\"\n",
    "     return [claim for claim in self.claims\n",
    "             if claim.procedure_code.startswith('L')]\n",
    " \n",
    " def create_evaluation_dataset(self, sample_size: int = 100) -> List[ClaimLine]:\n",
    "     \"\"\"Create a balanced evaluation dataset.\"\"\"\n",
    "     # For orthotic claims, use a mix of approved and denied claims\n",
    "     orthotic_claims = self.get_orthotic_claims()\n",
    "     \n",
    "     if not orthotic_claims:\n",
    "         logger.warning(\"No orthotic claims found in dataset\")\n",
    "         return []\n",
    "     \n",
    "     # Ensure we don't sample more than available\n",
    "     sample_size = min(sample_size, len(orthotic_claims))\n",
    "     \n",
    "     # Create a balanced sample with both approved and denied claims if available\n",
    "     approved_claims = [c for c in orthotic_claims if c.approval_status == \"APPROVED\"]\n",
    "     denied_claims = [c for c in orthotic_claims if c.approval_status == \"DENIED\"]\n",
    "     \n",
    "     print(f\"Available for sampling: {len(approved_claims)} approved, {len(denied_claims)} denied\")\n",
    "     \n",
    "     # If we have both types, create a balanced sample\n",
    "     if approved_claims and denied_claims:\n",
    "         # Determine how many of each to sample\n",
    "         approved_sample_size = min(sample_size // 2, len(approved_claims))\n",
    "         denied_sample_size = min(sample_size - approved_sample_size, len(denied_claims))\n",
    "         \n",
    "         # Adjust approved sample if we couldn't get enough denied samples\n",
    "         if denied_sample_size < (sample_size // 2):\n",
    "             approved_sample_size = min(sample_size - denied_sample_size, len(approved_claims))\n",
    "         \n",
    "         # Sample claims\n",
    "         sampled_approved = random.sample(approved_claims, approved_sample_size)\n",
    "         sampled_denied = random.sample(denied_claims, denied_sample_size)\n",
    "         \n",
    "         # Combine samples\n",
    "         sampled_claims = sampled_approved + sampled_denied\n",
    "         random.shuffle(sampled_claims)\n",
    "     else:\n",
    "         # If we only have one type, sample from what's available\n",
    "         available_claims = approved_claims or denied_claims\n",
    "         sampled_claims = random.sample(available_claims, sample_size)\n",
    "     \n",
    "     print(f\"Created evaluation dataset with {len(sampled_claims)} claims\")\n",
    "     print(f\"  - Approved: {sum(1 for c in sampled_claims if c.approval_status == 'APPROVED')}\")\n",
    "     print(f\"  - Denied: {sum(1 for c in sampled_claims if c.approval_status == 'DENIED')}\")\n",
    "     \n",
    "     return sampled_claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a0a98b-0313-4054-bda1-9ffc84fa91ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ambulance_dataset = pd.DataFrame(MedicareClaimsDataset('/Workspace/Users/benjamin.wynn@peraton.com/GlobalEditsModel/testing/test_lines/fps17_labeled_clmlines.csv').claims)\n",
    "orthotics_dataset = pd.DataFrame(MedicareClaimsDataset('/Workspace/Users/benjamin.wynn@peraton.com/GlobalEditsModel/testing/test_lines/fps88_labeled_clmlines.csv').claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b589ad-cd00-4339-a493-c58d62fdb97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/Workspace/Users/benjamin.wynn@peraton.com/GlobalEditsModel/testing/test_policy/fps88_policy.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_text = file.read()\n",
    "\n",
    "print(markdown_text)\n",
    "orthotics_dataset[\"policy\"] = markdown_text\n",
    "orthotics_dataset[\"edit\"] = 88\n",
    "with open('/Workspace/Users/benjamin.wynn@peraton.com/GlobalEditsModel/testing/test_policy/fps17_policy.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_text = file.read()\n",
    "\n",
    "print(markdown_text)\n",
    "ambulance_dataset[\"policy\"] = markdown_text\n",
    "ambulance_dataset[\"edit\"] = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07709726-312f-4417-921c-8e56cea2ece1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined = pd.concat([ambulance_dataset, orthotics_dataset])\n",
    "combined.to_csv('/Workspace/Users/benjamin.wynn@peraton.com/GlobalEditsModel/testing/testing_data.csv')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9352a6e4-ec87-4cfc-a44c-867f976366e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum(combined['revenue_code'] != \"None\") / combined.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7314b5-3c96-4edc-8944-7bf67ddf7639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rev_val = combined.iloc[0]['revenue_code']\n",
    "sum(combined['revenue_code'].notnull()) / combined.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0cd18b1-626c-46a2-af33-39d01c0491eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(combined['revenue_code'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d9c5c0-edac-412f-be50-a5e0a54e2610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7639815859569151,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing-Dataset-Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
